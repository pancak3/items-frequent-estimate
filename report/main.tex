\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, left=25mm, top=25mm, right=25mm, bottom=25mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{minted}
\usepackage[english]{babel,isodate}
\usepackage[pdftex, pdfauthor={Qifan Deng},
 pdftitle={COMP90056 - Stream Computing and Applications 2020, Assignment 2}, 
 pdfsubject={COMP90056 Assignment}]{hyperref}
\usepackage{pgfplots} 
\usepackage{SIunits}        % <-- required in preamble
\pgfplotsset{compat=newest} % 
\usepackage[justification=centering]{caption}
\usepackage{minted}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{epstopdf}
\usepackage{float}

\newcommand\gauss[2]{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\setlength{\columnsep}{20pt}
\setlength{\parindent}{8pt}
\setlength{\parskip}{3pt}

\pgfplotsset{compat=1.16}

\title{COMP90056 - Stream Computing and Applications 2020, Assignment 2 
\\Frequent Items in a Data Stream}
\author{
  Qifan Deng (1077479)\\
  \texttt{qifand@student.unimelb.edu.au} }
\date{\printdayoff\normalsize\today}

\begin{document}
\sloppy
% \twocolumn
\maketitle

\section{Introduction}
Three algorithms are implemented in this report to estimate the most frequent items of a data stream.
They are StickySampling \cite{stickylossy}, LossyCounting \cite{stickylossy} and SpaceSaving \cite{spacesaving}. 
% There is also a Baseline algorithm implemented to investigate the performance of the three algorithms.

The structure of this report is as follows. 
Section~\ref{preparation} introduces the set-up including hardware, operating system, environments 
and data stream preparation.
Section~\ref{implementations} presents the implementations of algorithms 
and the theoretical performance of them.
Section~\ref{resultsanalysis} provides results and analysis. 
Finally, Section~\ref{conclusion} concludes this report,
gives recommendations and lists possible further improvements. 

\section{Hardware \& Environments \& Data Stream}\label{preparation}

\paragraph{Hardware}
The experiments in section are conducted on a machine with the following specifications,
\begin{itemize}
     \setlength\itemsep{1pt}
       \item CPU: 1.8 GHz Quad-Core Intel Core i5
       \item Memory: 8 GB 2133 MHz LPDDR3
       \item Disk: WDC PC SN720 SDAPNTW-512G-1127 SSD
\end{itemize}
\paragraph{Environment}
The testing operating system is \texttt{macOS Catalina version 10.15.7 (19H2)}.
All the algorithms are implemented in \texttt{Python 3.8.5}.
The requirements of the algorithms are stored in \texttt{requirements.txt} which can be installed with command \texttt{pip install -r requirements.txt}.

\paragraph{Repeat Tests}
Once the environments are prepared, run \texttt{python3 main.py} under the source code folder to get all the figures in this report.

\paragraph{Data Stream}
The data stream is power-law distribution where 
the $i^{th}$ most frequent item has probability $\frac{1}{i^{z} \cdot{} Zeta(z)}$
where $z$ is a positive real-value parameter and $Zeta$ is \texttt{Riemann or Hurwitz zeta function} \cite{zeta}. 
Figure~\ref{powerlaw} shows the distributions when $z = \{1.1, 1.4, 1.7, 2.0\}$ and the data stream size is $10^6$.
As it shows, they have almost the ideal trend except for when $z$ is close to 1. 
This proves that data streams in this report are desired.
Besides, as Figure~\ref{powerlaw} annotates, there are $717130$ items has frequency at least 1\% when $z=1.1$,  
$819018$ when  $z=1.4$, $880423$ when $z=1.7$ and $924202$ when $z=2.0$.


\begin{figure}[H]
     \begin{subfigure}[b]{0.5\textwidth}
          \centering
          \resizebox{\linewidth}{!}{\includegraphics{eps/zipf-1.1-100-stream-1000000.eps}}
          \label{power-law-z-1.1-100-stream-1000000}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
          \centering
          \resizebox{\linewidth}{!}{\includegraphics{eps/zipf-1.4-100-stream-1000000.eps}}
          \label{power-law-z-1.4-100-stream-1000000}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
          \centering
          \resizebox{\linewidth}{!}{\includegraphics{eps/zipf-1.7-100-stream-1000000.eps}}
          \label{power-law-z-1.7-100-stream-1000000}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
          \centering
          \resizebox{\linewidth}{!}{\includegraphics{eps/zipf-2.0-100-stream-1000000.eps}}
          \label{power-law-z-2.0-100-stream-1000000}
    \end{subfigure}
 
    \caption{Power-law distribution with $z = \{1.1, 1.4, 1.7, 2.0\}$ , $10^2$ distinct items and $10^6$ items in stream.}
    \label{powerlaw}
\end{figure}

\section{Implementations}\label{implementations}
% \paragraph{Baseline}
% Baseline algorithm is simple to implement. The algorithm holds counters $C$.
% The $i^{th}$ item $x$ in data stream with $N$ items is called entry $<x, f>$. 
% If $x$ is not in $C$ , set $C_n$ to $<x, 1>$. Otherwise, increment $C_x$ to $<x, f+1>$.
% When user requests with support $s$, return the entries with $f$ greater than $f\cdot{}N$.
\paragraph{StickySampling \& LossyCounting \& SpaceSaving}
These three algorithms are implemented following the respective papers \cite{stickylossy,spacesaving}. 
The theoretical performance of them should be what Table~\ref{theoretical_performance} shows.

\subsection{Theoretical Performance \& Justification}
\begin{table}[H]
     \centering
      \begin{tabular}{|| c | c| c| c||} 
      \hline
       & StickySampling & LossyCounting & SpaceSaving \\ [0.5ex] 
      \hline\hline
      Update Time  &  $O(20s^{-1}log(100s^{-1}))$ & $O(10s^{-1}log(10^{-1}sN))$ & $O(log(s^{-1}))$ \\
      \hline
      Memory & $O(20s^{-1}(log100s^{-1}))$ & $O(10s^{-1}log(10^{-1}sN))$& $O(s^{-1})$ \\ 
      \hline
      Accuracy & $O(s)$ & $O(s)$ & $O(s)$ \\
      \hline
      \end{tabular}
     \caption{Theoretical performance of, StickySampling, LossyCounting and SpaceSaving;
      $s$ is support, $N$ is the current lenth of the stream and
      $n$ is the number of distinct items in stream.}
      \label{theoretical_performance}
\end{table}

% \paragraph{Baseline}
% The update time of Baseline is $O(1)$ because it just increments the corresponding $f$ with no other operation when updates.
% Its memory cost is $O(n)$ since it stores every distinct item. And accuracy is 100\% obviously.
\paragraph{StickySampling}
In a power-law distribution data stream, the probability of occurrences drops rapidly at the beginning of the increasing order. In this kind of distribution, StickySampling algorithm works. 
This is because items with high probabilities tend to appear earlier when the chance ($1/t$) to be selected is higher ($t$ increases thus $1/t$ drops).
After being selected, they also appear more frequently. Thus, they accumulate higher frequencies to against frequency diminishing at the timepoint of rate changing.
However, items with lower probabilities have a lower chance to be selected. 
Even they are selected, they have a higher chance to be evicted at the diminishing moment.
As the algorithm processing, items with higher probabilities are finally kept.

The memory cost of StickySampling is $O(\frac{2}{\epsilon}log(s^{-1}\delta^{-1})$ \cite{stickylossy}
($\epsilon{}$ is chosen to be $s/10$ and $\delta{}$ is chosen to be 0.01).
Its update time is also $O(\frac{2}{\epsilon}log(s^{-1}\delta^{-1})$ because the counters are iterated when the rate $r$ changes.
\paragraph{LossyCounting}
The memory cost of StickySampling is $O(\frac{1}{\epsilon}log(\epsilon{}N))$. 
% because the chance of removing counter decreases as items coming when the items distribution is power-law, i.e., 
% new items have decresing chances to remove a counter at boundaries.
Its update time is then $O(\frac{1}{\epsilon}log(\epsilon{}N))$ because it iterates all counters at boundaries.
\paragraph{SpaceSaving}
SpaceSaving has a fixed number of counters which is $m = 1/s$, so its memory cost is $O(1/s)$. 
And the update time is $O(log(1/s))$ because the QuickSort is used when updates.

\section{Results \& Analysis} \label{resultsanalysis}

\subsection{Memory \& Runtime \& Support}
Support $s$ from $10^{-5}$ to $10^{-2}$ were tested for the three algorithms.
Figure~\ref{MaxTrackedSupport} shows memory vs support. 
The maximum number of tracked items is used to present memory of the algorithms.
Figure~\ref{RuntimeSupport} shows runtime vs support.
Finally, Figure~\ref{MaxTrackedRuntime} shows memory vs runtime 
where the data is from the tests Figure~\ref{MaxTrackedSupport} and \ref{RuntimeSupport}.

\begin{figure}[H]
      \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{eps/MaxTracked-Support-zipf-2.0-10000-delta-0.01-stream-100000.eps}
        \caption{Maximum Number of Tracked Items vs Support; $z=2.0$, $10^4$ distinct items and $10^5$ items in stream.}
        \label{MaxTrackedSupport}
      \end{minipage}\hfill
      \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{eps/Runtime-Support-zipf-2.0-10000-delta-0.01-stream-100000.eps}
        \caption{Runtime vs Support; $z=2.0$, $10^4$ distinct items and $10^5$ items in stream.}
        \label{RuntimeSupport}
      \end{minipage}
      \centering
      \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{eps/MaxTracked-Runtime-zipf-2.0-10000-delta-0.01-stream-100000.eps}
        \caption{Maximum Number of Tracked Items vs Runtime; $z=2.0$, $10^4$ distinct items and $10^5$ items in stream.}
        \label{MaxTrackedRuntime}
      \end{minipage}
\end{figure}
Figure~\ref{MaxTrackedSupport} indicates that they all have a fixed memory.
Because as the items coming, they store most of the items that have high probabilities.
But for SpaceSaving, the fixed memory is $1/s$, so it drops when $s$ increases. 
Thus, it cannot store the most frequent items at some point. 

Figure~\ref{RuntimeSupport} indicates that SpaceSaving has a slightly increasing trend of runtime when
support increases. This is because it has more counters to sort when $s$ is greater.
However, the runtime of StickySampling and LossyCounting do not change much.
This can be explained by the update time row of Table~\ref{theoretical_performance}.
As Table~\ref{theoretical_performance} shows, the increment of $s$ does not affect the runtime of StickySampling much. And for LossyCounting, stream size $N$
affects much more than $s$. In addition, the runtime of StickySampling and SpaceSaving are not affected by $N$, 
LossyCounting has higher runtime than them.

Figure~\ref{MaxTrackedRuntime} indicates that memory of StickySampling and LossyCounting
do not have a correlation with the runtime. 

\subsection{Precision \& Runtime \& Skew}

\begin{figure}[H]
      \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{eps/Precision-Skew-s-0.0001-zipf-1000-delta-10000.0-stream-100000.eps}
        \caption{Precision vs Skew; $s=0.0001$, $10^3$ distinct items and $10^5$ items in stream.}
        \label{PrecisionSkew}
      \end{minipage}\hfill
      \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{eps/Runtime-Skew-s-0.0001-zipf-1000-delta-10000.0-stream-100000.eps}
        \caption{Runtime vs Skew; $s=0.0001$, $10^3$ distinct items and $10^5$ items in stream.}
        \label{RuntimeSkew}
      \end{minipage}
\end{figure}

Figure~\ref{PrecisionSkew} indicates that 
StickySampling always has precision close to 1 no matter how the 
stream distribution varies. 
Besides, LossyCounting also has a similar precision but is less stable.
On the contrary, SpaceSaving has much lower precision but the precision
increases as $z$ increasing.

Figure~\ref{RuntimeSkew} indicates that all three algorithms run faster
when $z$ is greater. This is because counters evicting happens less
when new items appear less in such distributions. 
The figure also shows that, when $z$ is big enough, SpaceSaving is faster than StickySampling, 
and StickySampling is faster than LossyCounting. 
However, when $z$ is small, SpaceSaving is much slower than the other two when they have
similar. 
This is because when new items come more frequently ($z$ is small),
SpaceSaving has to modify its counter more frequently which involves sorting counters.

\subsection{Average Relative Error vs Support}
Figure~\ref{AverageRelativeErrorSupport} shows the average relative error of StickySampling, LossyCounting and SpaceSaving.

 \begin{figure}[H]
      \centering
     \begin{subfigure}[b]{0.6\textwidth}
      \centering
      \resizebox{\linewidth}{!}{\includegraphics{eps/AverageRelativeError-Support-zipf-2.0-100-delta-0.01-stream-100000.eps}} 
      \end{subfigure}
      \caption{Average Relative Error vs Support; $z$ = 2.0, $10^2$ distinct items and $10^5$ items in stream.}  
      \label{AverageRelativeErrorSupport}
 \end{figure}
 It indicates that 
 LossyCounting always records the right count of an item.
 SpaceSaving also records the right count when $s$ is proper. 
 When $s$ is greater than some value, its estimates become worse.
 StickySampling has a logarithmic trend of the average relative error.
 This is because the higher $s$ is, the fewer counters it has 
 and it is a logarithmic formula which can be seen in Table~\ref{theoretical_performance}. 
 Thus, the average relative error becomes smaller.

 \section{Conclusion}\label{conclusion}
 In practice, unexpected situations always happen. 
 For example, in \cite{stickylossy}, LossyCounting is said to be better than StickySampling in practice,
 however, in the practice of this report, LossyCounting is worse than StickySampling both in practice and theoretically.
 This may because $\epsilon{} = s/10$ in this report but \cite{stickylossy} dose not. 
 Better performance of LossyCounting may be achieved by trying different $\epsilon$.
 Furthermore, the runtime tests on the three algorithms sometimes vary a lot even on the same data stream.
 This is because the computing resources of an operating system vary even during a short period of time.

 StickySampling, LossyCounting and SpaceSaving are investigated in this report and
 they all have different merits and shortcomings. 
 If the number of distinct items is known(estimable) and space is very lacked,
 SpaceSaving is the best choice since it uses a few spaces and has better performance with the proper number of fixed counters.
 Otherwise, if the number is invisible, LossyCounting gives a very high and stable precision but it is a little slower than StickySampling and also uses more space. If one prefers a stable precision and 
 does not mind the runtime and space difference, LossyCounting is a better choice than StickySampling.

 There are three possible improvements of this report. 
 The first, the tests, comparisons and analysis of the three algorithms against a baseline can be made.
 The second, all the tests should be taken multiple times on the same machine.
 During the testing, the operating system should not have other computing tasks to avoid resources competition.
 The last, large numbers of distinct items in the stream should be tested to presents the outstanding performance of SpaceSaving. This is not tested because the testing machine does not have high performance.
  
 \clearpage
\bibliographystyle{IEEEtran}
\bibliography{main}
\end{document}
